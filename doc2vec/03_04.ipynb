{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# doc2vec: How To Prep Document Vectors For Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Our Own Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data, clean it, split it into train/test, and then train a doc2vec model\n",
    "import gensim\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "messages = pd.read_csv('../../../data/spam.csv', encoding='latin-1')\n",
    "messages = messages.drop(labels = [\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis = 1)\n",
    "messages.columns = [\"label\", \"text\"]\n",
    "messages['text_clean'] = messages['text'].apply(lambda x: gensim.utils.simple_preprocess(x))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(messages['text_clean'],\n",
    "                                                    messages['label'], test_size=0.2)\n",
    "\n",
    "tagged_docs_tr = [gensim.models.doc2vec.TaggedDocument(v, [i]) for i, v in enumerate(X_train)]\n",
    "\n",
    "d2v_model = gensim.models.Doc2Vec(tagged_docs_tr,\n",
    "                                  vector_size=50,\n",
    "                                  window=2,\n",
    "                                  min_count=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00789178,  0.00179726, -0.00867785, -0.02377464,  0.01347704,\n",
       "        0.01534288, -0.01276833, -0.0040705 , -0.01572244,  0.00372224,\n",
       "        0.01325616,  0.01769315,  0.00371708,  0.01176185,  0.02344394,\n",
       "       -0.00152305,  0.00475951,  0.00444851,  0.01137068, -0.00524353,\n",
       "       -0.00312748,  0.00836366,  0.00295096,  0.00052437, -0.01073263,\n",
       "        0.0056195 , -0.0064914 , -0.01604193,  0.01453729,  0.01601758,\n",
       "        0.00298155,  0.00949255, -0.00374035,  0.01031712,  0.00238959,\n",
       "        0.00439079, -0.01720384, -0.02651107, -0.01355453,  0.01799411,\n",
       "       -0.00503105,  0.00260428,  0.01283077, -0.01282513, -0.00190053,\n",
       "       -0.0080715 ,  0.0101283 ,  0.00883396, -0.00948556, -0.00480423],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What does a document vector look like again?\n",
    "d2v_model.infer_vector(['convert', 'words', 'to', 'vectors'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How do we prepare these vectors to be used in a machine learning model?\n",
    "vectors = [[d2v_model.infer_vector (words)] for words in X_test]\n",
    "\n",
    "'''\n",
    "Storing w2v vectors as arrays bc we needed to do element wise averaging across all of the arrays to create\n",
    "our single vector representation of a text message. Element-wise averaging is much easier to do with an array \n",
    "than a list. Secondly. document vectors are not deterministic, so these vectors are slightly different \n",
    "each time I run it.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-1.8864933e-03,  7.5172829e-03,  2.7195187e-03, -9.6630324e-03,\n",
       "         1.0285481e-02,  9.2690727e-03, -4.8824595e-03, -5.5104955e-03,\n",
       "        -1.2425218e-02,  9.8774373e-04, -9.0716407e-04,  2.3203853e-03,\n",
       "         1.1044599e-02,  1.1648269e-02,  1.1151540e-02, -5.3891726e-03,\n",
       "         7.5852433e-03,  1.1432619e-03, -2.4092987e-03,  5.4307231e-03,\n",
       "         4.0494613e-03, -9.0388861e-03, -8.6892759e-03, -8.3465257e-04,\n",
       "        -9.2695502e-04,  1.6014901e-03,  4.4474592e-03,  1.5120900e-03,\n",
       "         7.7567738e-03, -2.7301288e-03,  1.0844887e-03, -4.4334084e-03,\n",
       "        -7.6287156e-03,  3.0590678e-03,  4.5597740e-03, -1.7824781e-03,\n",
       "        -1.0330799e-02, -5.8183079e-03,  4.5669070e-03, -9.5598938e-05,\n",
       "        -3.4995831e-04, -7.6616248e-03, -1.0572328e-03, -9.6582826e-03,\n",
       "         2.1980891e-03,  8.2624739e-04,  7.0422245e-03,  5.5585732e-03,\n",
       "        -7.4954093e-03, -2.8866485e-03], dtype=float32)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors[0] #looks random to us, but there is a meaning that the model is learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
